#!/usr/bin/env python3
"""SONAR-SLT evaluation helper.

This script compares the translations generated by ``tools_infer_sonar_slt.py``
with the reference captions contained in the metadata CSV and reports several
machine-translation metrics that are commonly used for sign-language
translation benchmarks.

Example
-------
```
python tools_eval_sonar_slt.py \
  --preds runs/sonar_slt_infer/preds.csv \
  --references meta.csv \
  --split test \
  --output runs/sonar_slt_infer/metrics.json
```

The script expects the prediction file created by the inference utility
(``preds.csv`` with the columns ``id,video,lang,text``) and a metadata CSV that
contains at least ``id`` and ``text`` columns.  By default the LSA-T metadata
(``meta.csv``) is semicolon separated; the reader tries to auto-detect the
delimiter but explicit ``--preds-delimiter``/``--refs-delimiter`` switches are
available as well.

Output
------
Metrics are printed to STDOUT and, if ``--output`` is provided, stored in a
JSON document containing both the aggregated scores and a few bookkeeping
fields (number of samples, missing references, ...).
"""

from __future__ import annotations

import argparse
import csv
import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

LOGGER = logging.getLogger("tools_eval_sonar_slt")
if not logging.getLogger().hasHandlers():
    logging.basicConfig(level=logging.INFO)


try:  # Optional but strongly recommended dependency
    import sacrebleu  # type: ignore[import]
except Exception:  # pragma: no cover - soft dependency
    sacrebleu = None  # type: ignore[assignment]


DEFAULT_METRICS: Tuple[str, ...] = ("bleu", "chrf", "ter", "rougeL", "wer")


@dataclass
class Example:
    """Aligned prediction/reference pair."""

    clip_id: str
    reference: str
    prediction: str


def detect_delimiter(path: Path, requested: Optional[str]) -> str:
    """Return the delimiter used by ``path``.

    When ``requested`` is provided it is used verbatim; otherwise we attempt to
    infer the delimiter from the file contents, falling back to comma.
    """

    if requested and requested.lower() != "auto":
        return requested

    sample = path.read_text(encoding="utf-8", errors="ignore")[:4096]
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=[",", ";", "\t", "|"])
        LOGGER.debug("Detected delimiter '%s' for %s", dialect.delimiter, path)
        return dialect.delimiter
    except csv.Error:
        fallback = ";" if ";" in sample and "," not in sample else ","
        LOGGER.debug("Falling back to delimiter '%s' for %s", fallback, path)
        return fallback


def read_csv(path: Path, delimiter: Optional[str]) -> List[Dict[str, str]]:
    if not path.exists():
        raise FileNotFoundError(f"CSV file not found: {path.resolve()}")
    delim = detect_delimiter(path, delimiter)
    with path.open("r", encoding="utf-8", newline="") as fh:
        reader = csv.DictReader(fh, delimiter=delim)
        rows = [dict(row) for row in reader]
    LOGGER.info("Loaded %d rows from %s", len(rows), path)
    return rows


def normalise_text(text: str, *, lowercase: bool) -> str:
    text = text.strip()
    if lowercase:
        text = text.lower()
    # Collapse consecutive whitespace to a single space
    return " ".join(text.split())


def align_examples(
    predictions: Sequence[Dict[str, str]],
    references: Sequence[Dict[str, str]],
    *,
    id_column: str,
    text_column: str,
    split_column: Optional[str],
    split_name: Optional[str],
    lowercase: bool,
) -> Tuple[List[Example], List[str], List[str]]:
    refs_by_id: Dict[str, Dict[str, str]] = {}
    for row in references:
        clip_id = row.get(id_column)
        if not clip_id:
            continue
        if split_name and split_column and row.get(split_column) != split_name:
            continue
        refs_by_id[clip_id] = row

    examples: List[Example] = []
    missing_refs: List[str] = []
    missing_preds: List[str] = []

    for row in predictions:
        clip_id = row.get(id_column)
        if not clip_id:
            continue
        ref_row = refs_by_id.get(clip_id)
        if ref_row is None:
            missing_refs.append(clip_id)
            continue
        ref_text = ref_row.get(text_column)
        pred_text = row.get(text_column)
        if ref_text is None:
            raise KeyError(f"Reference row for {clip_id!r} lacks column {text_column!r}")
        if pred_text is None:
            raise KeyError(f"Prediction row for {clip_id!r} lacks column {text_column!r}")
        examples.append(
            Example(
                clip_id=clip_id,
                reference=normalise_text(ref_text, lowercase=lowercase),
                prediction=normalise_text(pred_text, lowercase=lowercase),
            )
        )

    predicted_ids = {row.get(id_column) for row in predictions if row.get(id_column)}
    missing_preds = [clip_id for clip_id in refs_by_id if clip_id not in predicted_ids]

    return examples, missing_refs, missing_preds


def _require_sacrebleu(metric_name: str) -> None:
    if sacrebleu is None:  # pragma: no cover - guarded by tests
        raise RuntimeError(
            f"Metric '{metric_name}' requires the 'sacrebleu' package. "
            "Install SLT-v2 with the 'metrics' extra or pip install sacrebleu."
        )


def compute_bleu(examples: Sequence[Example]) -> float:
    _require_sacrebleu("bleu")
    sys_stream = [ex.prediction for ex in examples]
    ref_stream = [[ex.reference for ex in examples]]
    bleu = sacrebleu.corpus_bleu(sys_stream, ref_stream)
    return float(bleu.score)


def compute_chrf(examples: Sequence[Example], *, beta: float = 2.0) -> float:
    _require_sacrebleu("chrf")
    sys_stream = [ex.prediction for ex in examples]
    ref_stream = [[ex.reference for ex in examples]]
    chrf = sacrebleu.corpus_chrf(sys_stream, ref_stream, beta=beta)
    return float(chrf.score)


def compute_ter(examples: Sequence[Example]) -> float:
    _require_sacrebleu("ter")
    sys_stream = [ex.prediction for ex in examples]
    ref_stream = [[ex.reference for ex in examples]]
    ter = sacrebleu.corpus_ter(sys_stream, ref_stream)
    return float(ter.score)


def lcs_length(x: Sequence[str], y: Sequence[str]) -> int:
    if not x or not y:
        return 0
    # Dynamic programming with reduced memory footprint
    prev = [0] * (len(y) + 1)
    for x_tok in x:
        curr = [0]
        for j, y_tok in enumerate(y, start=1):
            if x_tok == y_tok:
                curr.append(prev[j - 1] + 1)
            else:
                curr.append(max(curr[-1], prev[j]))
        prev = curr
    return prev[-1]


def compute_rouge_l(examples: Sequence[Example], *, beta: float = 1.2) -> float:
    scores: List[float] = []
    for ex in examples:
        ref_tokens = ex.reference.split()
        pred_tokens = ex.prediction.split()
        if not ref_tokens and not pred_tokens:
            scores.append(1.0)
            continue
        if not ref_tokens or not pred_tokens:
            scores.append(0.0)
            continue
        lcs = lcs_length(ref_tokens, pred_tokens)
        prec = lcs / len(pred_tokens)
        rec = lcs / len(ref_tokens)
        if prec == 0.0 and rec == 0.0:
            scores.append(0.0)
            continue
        beta_sq = beta ** 2
        score = (1 + beta_sq) * prec * rec / (rec + beta_sq * prec)
        scores.append(score)
    return float(sum(scores) / len(scores) * 100)


def _levenshtein_distance(ref: Sequence[str], hyp: Sequence[str]) -> int:
    if not ref:
        return len(hyp)
    if not hyp:
        return len(ref)
    prev = list(range(len(hyp) + 1))
    for i, ref_tok in enumerate(ref, start=1):
        curr = [i]
        for j, hyp_tok in enumerate(hyp, start=1):
            cost = 0 if ref_tok == hyp_tok else 1
            curr.append(
                min(
                    curr[-1] + 1,  # insertion
                    prev[j] + 1,  # deletion
                    prev[j - 1] + cost,  # substitution
                )
            )
        prev = curr
    return prev[-1]


def compute_wer(examples: Sequence[Example]) -> float:
    total_edits = 0
    total_ref_tokens = 0
    for ex in examples:
        ref_tokens = ex.reference.split()
        hyp_tokens = ex.prediction.split()
        total_edits += _levenshtein_distance(ref_tokens, hyp_tokens)
        total_ref_tokens += max(1, len(ref_tokens))
    return float(total_edits / total_ref_tokens * 100)


def compute_metrics(examples: Sequence[Example], names: Iterable[str]) -> Dict[str, float]:
    results: Dict[str, float] = {}
    for name in names:
        key = name.lower()
        if key == "bleu":
            results["bleu"] = compute_bleu(examples)
        elif key in {"chrf", "chrf++"}:
            results["chrf"] = compute_chrf(examples)
        elif key == "ter":
            results["ter"] = compute_ter(examples)
        elif key.lower() in {"rouge", "rougel", "rouge-l"}:
            results["rougeL"] = compute_rouge_l(examples)
        elif key == "wer":
            results["wer"] = compute_wer(examples)
        else:
            raise ValueError(f"Unknown metric: {name}")
    return results


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate SONAR-SLT predictions")
    parser.add_argument("--preds", type=Path, required=True, help="CSV with model predictions")
    parser.add_argument("--references", type=Path, required=True, help="CSV with reference texts")
    parser.add_argument(
        "--preds-delimiter",
        type=str,
        default="auto",
        help="Delimiter for the predictions CSV (default: auto-detect)",
    )
    parser.add_argument(
        "--refs-delimiter",
        type=str,
        default="auto",
        help="Delimiter for the reference CSV (default: auto-detect)",
    )
    parser.add_argument(
        "--id-column",
        type=str,
        default="id",
        help="Shared identifier column in both CSV files (default: id)",
    )
    parser.add_argument(
        "--text-column",
        type=str,
        default="text",
        help="Column containing the textual transcription (default: text)",
    )
    parser.add_argument(
        "--split-column",
        type=str,
        default="split",
        help="Column used to filter a split in the reference CSV (default: split)",
    )
    parser.add_argument(
        "--split",
        type=str,
        default=None,
        help="If provided, evaluate only rows matching this split name",
    )
    parser.add_argument(
        "--metrics",
        type=str,
        nargs="*",
        default=list(DEFAULT_METRICS),
        help="Metrics to compute (default: %(default)s)",
    )
    parser.add_argument(
        "--lowercase",
        action="store_true",
        help="Lowercase text before scoring (default: keep original casing)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional path to store the metrics JSON",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    preds_rows = read_csv(args.preds, args.preds_delimiter)
    refs_rows = read_csv(args.references, args.refs_delimiter)

    examples, missing_refs, missing_preds = align_examples(
        preds_rows,
        refs_rows,
        id_column=args.id_column,
        text_column=args.text_column,
        split_column=args.split_column,
        split_name=args.split,
        lowercase=args.lowercase,
    )

    if not examples:
        raise RuntimeError("No overlapping predictions and references were found")

    metrics = compute_metrics(examples, args.metrics)

    print(f"Evaluated {len(examples)} samples")
    width = max(len(name) for name in metrics) if metrics else 10
    for name, value in metrics.items():
        print(f"{name.rjust(width)}: {value:.2f}")

    if missing_refs:
        LOGGER.warning("%d predictions missing references", len(missing_refs))
    if missing_preds:
        LOGGER.warning("%d references missing predictions", len(missing_preds))

    if args.output:
        payload = {
            "samples": len(examples),
            "metrics": metrics,
            "missing_references": missing_refs,
            "missing_predictions": missing_preds,
            "config": {
                "lowercase": args.lowercase,
                "metrics": args.metrics,
                "split": args.split,
            },
        }
        args.output.parent.mkdir(parents=True, exist_ok=True)
        args.output.write_text(json.dumps(payload, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        LOGGER.info("Saved metrics to %s", args.output)


if __name__ == "__main__":  # pragma: no cover - manual invocation
    main()

